{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 12:54:02 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.87.132.44 instead (on interface en0)\n",
      "24/11/06 12:54:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/06 12:54:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql as sparksession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "spark = sparksession.SparkSession.builder.appName('Coffeshop').getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#read the csv file\n",
    "df=spark.read.csv(\"./data/Coffee Shop Sales.csv\",header=True)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------------+---------------+--------+---------------+----------+----------+------------------+--------------------+--------------------+\n",
      "|transaction_id|transaction_date|transaction_time|transaction_qty|store_id| store_location|product_id|unit_price|  product_category|        product_type|      product_detail|\n",
      "+--------------+----------------+----------------+---------------+--------+---------------+----------+----------+------------------+--------------------+--------------------+\n",
      "|             1|      01/01/2023|        07:06:11|              2|       5|Lower Manhattan|        32|         3|            Coffee|Gourmet brewed co...|         Ethiopia Rg|\n",
      "|             2|      01/01/2023|        07:08:56|              2|       5|Lower Manhattan|        57|       3.1|               Tea|     Brewed Chai tea|Spicy Eye Opener ...|\n",
      "|             3|      01/01/2023|        07:14:04|              2|       5|Lower Manhattan|        59|       4.5|Drinking Chocolate|       Hot chocolate|   Dark chocolate Lg|\n",
      "|             4|      01/01/2023|        07:20:24|              1|       5|Lower Manhattan|        22|         2|            Coffee|         Drip coffee|Our Old Time Dine...|\n",
      "|             5|      01/01/2023|        07:22:41|              2|       5|Lower Manhattan|        57|       3.1|               Tea|     Brewed Chai tea|Spicy Eye Opener ...|\n",
      "+--------------+----------------+----------------+---------------+--------+---------------+----------+----------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transaction_id: bigint, transaction_date: bigint, transaction_time: bigint, transaction_qty: bigint, store_id: bigint, store_location: bigint, product_id: bigint, unit_price: bigint, product_category: bigint, product_type: bigint, product_detail: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check is there any null value in the data\n",
    "df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- transaction_qty: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- store_location: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- product_detail: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the data type of the columns\n",
    "df = df.withColumn('transaction_date', to_date(df['transaction_date'], 'dd/MM/yyyy'))\n",
    "df=df.withColumn('transaction_qty', col('transaction_qty').cast('int'))\n",
    "df=df.withColumn('store_id', col('store_id').cast('int'))\n",
    "df=df.withColumn('product_id', col('product_id').cast('int'))\n",
    "df=df.withColumn('unit_price', col('unit_price').cast('float'))\n",
    "df=df.withColumn('transaction_id', col('transaction_id').cast('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- transaction_qty: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- store_location: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- unit_price: float (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- product_detail: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row number 149116  column number 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count the number of rows in the dataframe\n",
    "print('row number',df.count(),' column number',len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# find dupplicate rows\n",
    "duplicate_count=df.groupBy(df.columns).count().filter(\"count>1\").agg({\"count\":\"sum\"}).collect()[0][0]\n",
    "print(duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('coffeeshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|      day|total_sales|\n",
      "+---------+-----------+\n",
      "|   Monday|  101677.28|\n",
      "|   Friday|   101373.0|\n",
      "| Thursday|  100767.78|\n",
      "|Wednesday|  100313.54|\n",
      "|  Tuesday|   99455.94|\n",
      "|   Sunday|   98330.31|\n",
      "| Saturday|   96894.48|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#which days of the week are the busiest?\n",
    "spark.sql(\"\"\"\n",
    "          SELECT date_format(transaction_date, 'EEEE') as day , round(sum(transaction_qty*unit_price),2) as total_sales FROM coffeeshop\n",
    "          group by 1\n",
    "          order by total_sales desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "| store_location|total_sales|\n",
      "+---------------+-----------+\n",
      "| Hell's Kitchen|  236511.17|\n",
      "|        Astoria|  232243.91|\n",
      "|Lower Manhattan|  230057.25|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#which store location has the highest sales?\n",
    "spark.sql(\"\"\"\n",
    "          SELECT store_location, round(sum(transaction_qty*unit_price),2) as total_sales FROM coffeeshop\n",
    "          group by store_location\n",
    "          order by total_sales desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "| store_location|      day|total_sales|\n",
      "+---------------+---------+-----------+\n",
      "|        Astoria|Wednesday|   34244.63|\n",
      "|        Astoria| Thursday|   34140.37|\n",
      "|        Astoria|   Monday|   33928.29|\n",
      "|        Astoria|   Friday|   33472.75|\n",
      "|        Astoria|   Sunday|   32795.18|\n",
      "|        Astoria| Saturday|   31845.93|\n",
      "|        Astoria|  Tuesday|   31816.76|\n",
      "| Hell's Kitchen|  Tuesday|   34846.47|\n",
      "| Hell's Kitchen|   Friday|   34743.18|\n",
      "| Hell's Kitchen|Wednesday|   33779.09|\n",
      "| Hell's Kitchen|   Sunday|   33685.63|\n",
      "| Hell's Kitchen| Thursday|   33468.06|\n",
      "| Hell's Kitchen|   Monday|   33389.51|\n",
      "| Hell's Kitchen| Saturday|   32599.23|\n",
      "|Lower Manhattan|   Monday|   34359.48|\n",
      "|Lower Manhattan| Thursday|   33159.35|\n",
      "|Lower Manhattan|   Friday|   33157.07|\n",
      "|Lower Manhattan|  Tuesday|   32792.71|\n",
      "|Lower Manhattan| Saturday|   32449.32|\n",
      "|Lower Manhattan|Wednesday|   32289.82|\n",
      "+---------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#which store location's busiest days ?\n",
    "spark.sql(\"\"\"\n",
    "          SELECT store_location,date_format(transaction_date, 'EEEE') as day , round(sum(transaction_qty*unit_price),2) as total_sales FROM coffeeshop\n",
    "          group by store_location,day\n",
    "          order by store_location,total_sales desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-----------+\n",
      "| store_location|product_category|total_sales|\n",
      "+---------------+----------------+-----------+\n",
      "|        Astoria|          Coffee|    89744.3|\n",
      "|        Astoria|             Tea|    67839.9|\n",
      "|        Astoria|          Bakery|   26599.75|\n",
      "| Hell's Kitchen|          Coffee|   91222.65|\n",
      "| Hell's Kitchen|             Tea|    64701.3|\n",
      "| Hell's Kitchen|          Bakery|   27386.95|\n",
      "|Lower Manhattan|          Coffee|    88985.5|\n",
      "|Lower Manhattan|             Tea|   63864.75|\n",
      "|Lower Manhattan|          Bakery|   28328.94|\n",
      "+---------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#what are the most sold 3 product category per store?\n",
    "spark.sql(\"\"\"\n",
    "         SELECT store_location, product_category ,total_sales from\n",
    "          (\n",
    "         SELECT store_location, product_category , round(sum(transaction_qty*unit_price),2) as total_sales \n",
    "          , rank() over(partition by store_location order by sum(transaction_qty*unit_price) desc) as rank\n",
    "            FROM coffeeshop\n",
    "          group by store_location, product_category\n",
    "          order by store_location,total_sales desc) as subquery\n",
    "          where rank <=3\n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------+\n",
      "| store_location|      product_detail|total_sales|\n",
      "+---------------+--------------------+-----------+\n",
      "|        Astoria|   Dark chocolate Lg|     7897.5|\n",
      "|        Astoria|Sustainably Grown...|    7509.75|\n",
      "|        Astoria|            Latte Rg|     6358.0|\n",
      "| Hell's Kitchen|           Civet Cat|     7380.0|\n",
      "| Hell's Kitchen|Sustainably Grown...|    7329.25|\n",
      "| Hell's Kitchen|   Dark chocolate Lg|     6534.0|\n",
      "|Lower Manhattan|   Dark chocolate Lg|     6574.5|\n",
      "|Lower Manhattan|            Latte Rg|    6387.75|\n",
      "|Lower Manhattan|Sustainably Grown...|    6312.75|\n",
      "+---------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#what are the most sold 3 product per store?\n",
    "spark.sql(\"\"\"\n",
    "         SELECT store_location, product_detail ,total_sales from\n",
    "          (\n",
    "         SELECT store_location, product_detail , round(sum(transaction_qty*unit_price),2) as total_sales \n",
    "          , rank() over(partition by store_location order by sum(transaction_qty*unit_price) desc) as rank\n",
    "            FROM coffeeshop\n",
    "          group by store_location, product_detail\n",
    "          order by store_location,total_sales desc) as subquery\n",
    "          where rank <=3\n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------+\n",
      "| store_location|      product_detail|total_sales|\n",
      "+---------------+--------------------+-----------+\n",
      "|        Astoria|Traditional Blend...|     277.45|\n",
      "|        Astoria|           Earl Grey|      268.5|\n",
      "|        Astoria|      Dark chocolate|       83.2|\n",
      "| Hell's Kitchen|  Serenity Green Tea|      407.0|\n",
      "| Hell's Kitchen|Spicy Eye Opener ...|     383.25|\n",
      "| Hell's Kitchen|I Need My Bean! D...|      360.0|\n",
      "|Lower Manhattan|          Peppermint|     331.15|\n",
      "|Lower Manhattan|         Lemon Grass|      268.5|\n",
      "|Lower Manhattan|      Dark chocolate|      249.6|\n",
      "+---------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#what are the least sold 3 product per store?\n",
    "spark.sql(\"\"\"\n",
    "         SELECT store_location, product_detail ,total_sales from\n",
    "          (\n",
    "         SELECT store_location, product_detail , round(sum(transaction_qty*unit_price),2) as total_sales \n",
    "          , rank() over(partition by store_location order by sum(transaction_qty*unit_price) asc) as rank\n",
    "            FROM coffeeshop\n",
    "          group by store_location, product_detail\n",
    "          order by store_location,total_sales desc) as subquery\n",
    "          where rank <=3\n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 13:41:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:41:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:41:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:41:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----+\n",
      "|      product_detail|unit_price|rank|\n",
      "+--------------------+----------+----+\n",
      "|           Civet Cat|      45.0|   1|\n",
      "| Organic Decaf Blend|      28.0|   2|\n",
      "|I Need My Bean! T...|      28.0|   2|\n",
      "|I Need My Bean! L...|      23.0|   3|\n",
      "|I Need My Bean! D...|      23.0|   3|\n",
      "| Organic Decaf Blend|      23.0|   3|\n",
      "|I Need My Bean! T...|      23.0|   3|\n",
      "+--------------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#what are the most expensive 3 products sold?\n",
    "spark.sql(\"\"\"\n",
    "        SELECT  distinct product_detail , unit_price,rank\n",
    "          FROM (\n",
    "         SELECT   product_detail , unit_price  , dense_rank() over (order by unit_price desc) as rank\n",
    "         FROM coffeeshop\n",
    "         ORDER BY  unit_price desc\n",
    "          ) as subquery\n",
    "        where rank <= 3\n",
    "          \n",
    "          \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----+\n",
      "|      product_detail|total_count|rank|\n",
      "+--------------------+-----------+----+\n",
      "|        Earl Grey Rg|       4708|   1|\n",
      "|   Dark chocolate Lg|       4668|   2|\n",
      "|Morning Sunrise C...|       4643|   3|\n",
      "+--------------------+-----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/06 13:44:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "#what are the most sold 3 products ?\n",
    "spark.sql(\"\"\"\n",
    "        SELECT   product_detail , total_count,rank\n",
    "          FROM (\n",
    "         SELECT   product_detail , sum(transaction_qty) as total_count  , dense_rank() over (order by sum(transaction_qty) desc) as rank\n",
    "         FROM coffeeshop\n",
    "         GROUP BY product_detail\n",
    "          ) as subquery\n",
    "        where rank <= 3\n",
    "          \n",
    "          \n",
    "          \"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
